{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c045b6f-2966-40da-bbf0-2cca52370a3b",
   "metadata": {},
   "source": [
    "## Llama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4b8c17-89bd-41d3-82fb-5999f9c08d83",
   "metadata": {},
   "source": [
    "This notebook shows how you can chat with Llama through an EMISSOR client. The EMISSOR layer will capture the interaction as a scenario for further analysis.\n",
    "For chatting with a LLama model, we use the [LangChain layer on top of Ollama](https://api.python.langchain.com/en/latest/chat_models/langchain_ollama.chat_models.ChatOllama.html). \n",
    "\n",
    "Ollama allows you to pull a model from the web to your local machine and use a ```chat``` function to send instructions to local model to get a response. \n",
    "\n",
    "```\n",
    "https://github.com/ollama/ollama\n",
    "```\n",
    "\n",
    "Instead of the Ollama ```chat``` function, we will create a client through ChatOllama so that we can set parameters for the behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35b76e5-6d8a-4091-b529-802049c5e39b",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26547e64-b644-44da-9685-592e73d528fb",
   "metadata": {},
   "source": [
    "The following dependencies need to be installed for this notebook. The dependencies are in the requirements.txt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa66457f-d098-46b4-aca3-876400dcc702",
   "metadata": {},
   "source": [
    "* pip install emissor\n",
    "* pip install cltl.combot\n",
    "* pip install ollama\n",
    "* pip install -U langchain-ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e213902-fd4d-4d5a-9d17-8c558af190db",
   "metadata": {},
   "source": [
    "There are various versions of models. We are going to pull the smallest Llama3.2 model that already gives reasonable performance but only works for text input.\n",
    "To be able to access the model through Ollama, we need to pull it from the terminal in the same virtual environment:\n",
    "\n",
    "```\n",
    "ollama pull llama3.2:1b\n",
    "```\n",
    "\n",
    "You only need to do this once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63864de-b1c2-4051-9a2f-23bc2dbb84ee",
   "metadata": {},
   "source": [
    "### Loading Llama in ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f54334a9-b0bc-440d-868d-924a772b9620",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "llama_model = \"llama3.2:1b\" ### 1B\n",
    "#llama_model = \"llama3.2\" ### 3B\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model = llama_model,\n",
    "    temperature = 0.8,\n",
    "    num_predict = 256,\n",
    "    # other params ...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e96a772-4705-4291-941e-909658eae39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct = { 'role': 'system', 'content': \"You are a docter and you will receive questions from patients. Be brief and no more than two sentences.\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7387c124-7772-4511-9afb-cb83449ca47a",
   "metadata": {},
   "source": [
    "### Creating an EMISSOR client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89a29313-9cd6-4321-9f0d-fd2746ae2ac6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from leolani_client import LeolaniChatClient\n",
    "emissor_path = \"./emissor\"\n",
    "HUMAN=\"Piek\"\n",
    "AGENT=\"Llama\"\n",
    "leolaniClient = LeolaniChatClient(emissor_path=emissor_path, agent=AGENT, human=HUMAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f437945-4b5c-418a-b9a1-55b7435d5da6",
   "metadata": {},
   "source": [
    "### Interaction loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fa6d0c3-a5fa-4a5a-b2f9-093ee36aba51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are a docter and you will receive questions from patients. Be brief and no more than two sentences.'}]\n",
      "Llama: <|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "I'm ready to help. Go ahead and ask your question.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " I have headache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Piek: I have headache\n",
      "Llama: Headaches can be caused by a variety of factors, including tension, stress, dehydration, or even sinus pressure. I recommend you try some over-the-counter pain relievers like acetaminophen or ibuprofen to see if that helps alleviate your symptoms.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " I did not sleep well\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Piek: I did not sleep well\n",
      "Llama: Tension and lack of sleep can definitely trigger headaches. Have you tried relaxation techniques, such as deep breathing exercises or meditation, to help calm your mind and body before bed?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " bye\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Piek: bye\n"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "history.append(instruct)\n",
    "print(history)\n",
    "### First prompt\n",
    "response = llm.invoke(history)\n",
    "utterance = response.content\n",
    "print(AGENT + \": \" + utterance)\n",
    "leolaniClient._add_utterance(AGENT, utterance) \n",
    "prompt = { 'role': 'system', 'content': utterance}\n",
    "history.append(prompt)\n",
    "\n",
    "utterance = input(\"\\n\")\n",
    "print(HUMAN + \": \" + utterance)\n",
    "leolaniClient._add_utterance(HUMAN, utterance)\n",
    "prompt = { 'role': 'user', 'content': utterance}\n",
    "history.append(prompt)\n",
    "\n",
    "while not (utterance.lower() == \"stop\" or utterance.lower() == \"bye\"):\n",
    "    # Create the response from the system and store this as a new signal\n",
    "    response = llm.invoke(history)\n",
    "    utterance = response.content\n",
    "    print(AGENT + \": \" + utterance)\n",
    "    leolaniClient._add_utterance(AGENT, utterance) \n",
    "    prompt = { 'role': 'system', 'content': utterance}\n",
    "    history.append(prompt)\n",
    "\n",
    "    utterance = input(\"\\n\")\n",
    "    print(HUMAN + \": \" + utterance)\n",
    "    leolaniClient._add_utterance(HUMAN, utterance)\n",
    "    prompt = { 'role': 'user', 'content': utterance}\n",
    "    history.append(prompt)\n",
    "\n",
    "##### After completion, we save the scenario in the defined emissor folder.\n",
    "leolaniClient._save_scenario() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1136a7a-2284-46ff-80b3-00247195e022",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
