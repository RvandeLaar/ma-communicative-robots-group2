{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd0cf80a-dcaa-4e9c-a7cf-4d9e4f50bccb",
   "metadata": {},
   "source": [
    "# Chat in an Ai2Thor space with EMISSOR\n",
    "\n",
    "In this notebook, we demonstrate how you can chat with the agent in the Ai2Thor space and save the so-called ```signals``` in EMISSOR.\n",
    "For this, we will import 1) a LeolaniChatClient so that we can capture the signals and store them in an EMISSOR scenario and 2) an Ai2ThorClient that let us interact with the space through an agent. There are three modalities considered:\n",
    "\n",
    "1. text modality for the turns of the user and the agent\n",
    "2. action modality for the actions caried out by the agent in the space (mostly navigation)\n",
    "3. image modality for the objects found by the agent in the space\n",
    "\n",
    "EMISSOR stores signals for each modality in a scenario folder. At the start of an interaction, a new scenario folder is created together with a JSON file with the same name, the so-called scenario JSON. The scenario JSON contains the meta data for the scenario as a whole, including a temporal ruler in terms of a start and end time. The temporal ruler is used to align the signal in a temporal sequence.\n",
    "\n",
    "https://ai2thor.allenai.org/ithor/documentation/environment-state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "876834eb-5322-4ce8-833c-db3676f4ac81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from leolani_client import LeolaniChatClient, Action\n",
    "emissor_path = \"./emissor\"\n",
    "HUMAN=\"Piek\"\n",
    "AGENT=\"Ai2Thor\"\n",
    "leolaniClient = LeolaniChatClient(emissor_path=emissor_path, agent=AGENT, human=HUMAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ecb5fc3-ff35-45fc-b31a-cd00d15c44d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ai2thor\n",
    "from ai2thor.controller import Controller\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "872ac6c3-0e2b-4813-b112-09f1c0864d85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ai2thor_client import Ai2ThorClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84ccbce3-e6a1-47e1-a91f-1cadd29b6915",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ai2Thor>Hi Piek. Tell me what to do.\n",
      "Ai2Thor>This is what I can do:('I can do the following:', \"['find', 'describe', 'move', 'go', 'turn', 'forward', 'back', 'left', 'right', 'open', 'close', 'look']\")\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Piek>  describe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ai2Thor>I see 67 things there.\n",
      "Apple\n",
      "Bottle\n",
      "\tI can break it.\n",
      "Bowl\n",
      "Bread\n",
      "ButterKnife\n",
      "Cabinet\n",
      "\tI can open it.\n",
      "Cabinet\n",
      "\tI can open it.\n",
      "Cabinet\n",
      "\tI can open it.\n",
      "Cabinet\n",
      "\tI can open it.\n",
      "Cabinet\n",
      "\tI can open it.\n",
      "Cabinet\n",
      "\tI can open it.\n",
      "CellPhone\n",
      "\tI can break it.\n",
      "Chair\n",
      "\tI can move it.\n",
      "Chair\n",
      "\tI can move it.\n",
      "CoffeeMachine\n",
      "\tI can move it.\n",
      "CounterTop\n",
      "CounterTop\n",
      "CounterTop\n",
      "CreditCard\n",
      "Cup\n",
      "\tI can break it.\n",
      "DishSponge\n",
      "Drawer\n",
      "\tI can open it.\n",
      "Drawer\n",
      "\tI can open it.\n",
      "Drawer\n",
      "\tI can open it.\n",
      "Egg\n",
      "\tI can break it.\n",
      "Faucet\n",
      "Floor\n",
      "Fork\n",
      "Fridge\n",
      "\tI can open it.\n",
      "GarbageCan\n",
      "\tI can move it.\n",
      "HousePlant\n",
      "\tI can move it.\n",
      "Knife\n",
      "Lettuce\n",
      "LightSwitch\n",
      "Microwave\n",
      "\tI can move it.\n",
      "\tI can open it.\n",
      "Mug\n",
      "\tI can break it.\n",
      "Pan\n",
      "PaperTowelRoll\n",
      "PepperShaker\n",
      "Plate\n",
      "\tI can break it.\n",
      "Pot\n",
      "Potato\n",
      "SaltShaker\n",
      "Shelf\n",
      "Shelf\n",
      "Shelf\n",
      "ShelvingUnit\n",
      "\tI can move it.\n",
      "Sink\n",
      "SinkBasin\n",
      "SoapBottle\n",
      "Spatula\n",
      "Spoon\n",
      "Statue\n",
      "\tI can break it.\n",
      "StoveBurner\n",
      "StoveBurner\n",
      "StoveBurner\n",
      "StoveBurner\n",
      "StoveKnob\n",
      "StoveKnob\n",
      "StoveKnob\n",
      "StoveKnob\n",
      "Toaster\n",
      "\tI can move it.\n",
      "Tomato\n",
      "Vase\n",
      "\tI can break it.\n",
      "Vase\n",
      "\tI can break it.\n",
      "Window\n",
      "\tI can break it.\n",
      "Window\n",
      "\tI can break it.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Piek>  bye\n"
     ]
    }
   ],
   "source": [
    "ai2ThorClient = Ai2ThorClient()\n",
    "\n",
    "max_context=50\n",
    "AI = \"AI\"\n",
    "\n",
    "utterance = \"Hi %s. Tell me what to do.\" % HUMAN\n",
    "print(AGENT+\">\"+utterance)\n",
    "leolaniClient._add_utterance(AGENT, utterance) \n",
    "\n",
    "utterance = \"This is what I can do:\"+str(ai2ThorClient.what_i_can_do())\n",
    "print(AGENT+\">\"+utterance)\n",
    "leolaniClient._add_utterance(AGENT, utterance) \n",
    "\n",
    "utterance = input(HUMAN+\"> \")\n",
    "leolaniClient._add_utterance(HUMAN, utterance) \n",
    "\n",
    "while not (utterance.lower() == \"stop\" or utterance.lower() == \"bye\"):\n",
    "        ai2ThorClient.process_instruction(utterance)\n",
    "        for utterance in ai2ThorClient._answers:\n",
    "            print(AGENT+\">\"+str(utterance))\n",
    "            leolaniClient._add_utterance(AGENT, utterance)\n",
    "            \n",
    "        for obj, objectType, coord, image in ai2ThorClient._perceptions:\n",
    "            leolaniClient._add_image(obj['name'], objectType, coord, image) \n",
    "\n",
    "        for action in ai2ThorClient._actions:\n",
    "            leolaniClient._add_action(action)\n",
    "            \n",
    "        utterance = input(HUMAN+\"> \")\n",
    "        leolaniClient._add_utterance(HUMAN, utterance) \n",
    "\n",
    "ai2ThorClient._controller.stop()\n",
    "##### After completion, we save the scenario in the defined emissor folder.\n",
    "leolaniClient._save_scenario() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c98e62-4c8f-492e-a409-4f02c472f6ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c14c56c-47eb-46c3-ad69-6fb3c808bfa0",
   "metadata": {},
   "source": [
    "## Modalities in EMISSOR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649b880c-c087-40d4-8bbc-ddceca8a4613",
   "metadata": {},
   "source": [
    "For each modality a separate JSON file is used. Since EMISSOR does not support actions, they are now stored as special text signals.\n",
    "Signals can have annotations. In the case of text signals, we annotate the source of the turn. The sources are the HUMAN, the AGENT or ACTION. In the former two cases, this means either of the two entered this turn, whereas the latter is used for actions carried out by Ai2Thor. The ```text.json``` thus contain a mixture of turns and actions. Here is an example of  turn as a text signal, where the value in the annotation is the speaker ```Piek```:\n",
    "\n",
    "```\n",
    "  {\n",
    "    \"@context\": {...},\n",
    "    \"@type\": \"TextSignal\",\n",
    "    \"id\": \"770630ea-3930-4d26-9d9c-27b62312f78c\",\n",
    "    \"ruler\": {...},\n",
    "    \"seq\": [...],\n",
    "    \"modality\": \"TEXT\",\n",
    "    \"time\": {...},\n",
    "    \"files\": [],\n",
    "    \"mentions\": [\n",
    "      {\n",
    "        \"@context\": {...},\n",
    "        \"@type\": \"Mention\",\n",
    "        \"id\": \"d9dbd4b9-d6a8-40c3-9ea6-e906ecd98100\",\n",
    "        \"segment\": [\n",
    "          {\n",
    "            \"@context\": {...},\n",
    "            \"@type\": \"Index\",\n",
    "            \"container_id\": \"770630ea-3930-4d26-9d9c-27b62312f78c\",\n",
    "            \"start\": 0,\n",
    "            \"stop\": 8,\n",
    "            \"_py_type\": \"emissor.representation.container-Index\"\n",
    "          }\n",
    "        ],\n",
    "        \"annotations\": [\n",
    "          {\n",
    "            \"@context\": {...},\n",
    "            \"@type\": \"Annotation\",\n",
    "            \"type\": \"ConversationalAgent\",\n",
    "            \"value\": \"Piek\",\n",
    "            \"source\": \"LEOLANI\",\n",
    "            \"timestamp\": 1730187274264,\n",
    "            \"_py_type\": \"emissor.representation.scenario-Annotation\"\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    ],\n",
    "    \"text\": \"find cup\"\n",
    "  },\n",
    "```\n",
    "\n",
    "\n",
    "Here is a text signal in which Ai2Thor reports what it found in a chat turn, so the value in the annotation is ```Ai2Thor```:\n",
    "\n",
    "```\n",
    "  {\n",
    "    \"@context\": {...},\n",
    "    \"@type\": \"TextSignal\",\n",
    "    \"id\": \"1d550a4a-52fa-484a-97dc-64665da699db\",\n",
    "    \"ruler\": {\n",
    "      \"@context\": {...},\n",
    "      \"@type\": \"Index\",\n",
    "      \"container_id\": \"1d550a4a-52fa-484a-97dc-64665da699db\",\n",
    "      \"start\": 0,\n",
    "      \"stop\": 135,\n",
    "      \"_py_type\": \"emissor.representation.container-Index\"\n",
    "    },\n",
    "    \"seq\": [...],\n",
    "    \"modality\": \"TEXT\",\n",
    "    \"time\": {...},\n",
    "      \"@type\": \"TemporalRuler\",\n",
    "      \"container_id\": \"<emissor.persistence.persistence.ScenarioController object at 0x10d858430>\",\n",
    "      \"start\": 1730187274333,\n",
    "      \"end\": 1730187274333\n",
    "    },\n",
    "    \"files\": [],\n",
    "    \"mentions\": [\n",
    "      {\n",
    "        \"@context\": {...},\n",
    "        \"@type\": \"Mention\",\n",
    "        \"id\": \"791475d7-17b6-48b0-974a-78f548486648\",\n",
    "        \"segment\": [\n",
    "          {\n",
    "            \"@context\": {...},\n",
    "            \"@type\": \"Index\",\n",
    "            \"container_id\": \"1d550a4a-52fa-484a-97dc-64665da699db\",\n",
    "            \"start\": 0,\n",
    "            \"stop\": 135,\n",
    "            \"_py_type\": \"emissor.representation.container-Index\"\n",
    "          }\n",
    "        ],\n",
    "        \"annotations\": [\n",
    "          {\n",
    "            \"@context\": {...},\n",
    "            \"@type\": \"Annotation\",\n",
    "            \"type\": \"ConversationalAgent\",\n",
    "            \"value\": \"Ai2Thor\",\n",
    "            \"source\": \"LEOLANI\",\n",
    "            \"timestamp\": 1730187274333,\n",
    "            \"_py_type\": \"emissor.representation.scenario-Annotation\"\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    ],\n",
    "    \"text\": \"I found 1 instances of type cup in my view\\nCup_8266e2aa at {'x': 1.0786449909210205, 'y': 0.8995606899261475, 'z': -0.7677611708641052}\"\n",
    "  },\n",
    "```\n",
    "\n",
    "The next examples shows an action as a text signal:\n",
    "\n",
    "```\n",
    "  {\n",
    "    \"@context\": {...},\n",
    "    \"@type\": \"TextSignal\",\n",
    "    \"id\": \"859064d1-5b01-4dbf-8f17-98d69d6e62db\",\n",
    "    \"ruler\": {\n",
    "      \"@context\": {...},\n",
    "      \"@type\": \"Index\",\n",
    "      \"container_id\": \"859064d1-5b01-4dbf-8f17-98d69d6e62db\",\n",
    "      \"start\": 0,\n",
    "      \"stop\": 4,\n",
    "      \"_py_type\": \"emissor.representation.container-Index\"\n",
    "    },\n",
    "    \"seq\": [...],\n",
    "    \"modality\": \"TEXT\",\n",
    "    \"time\": {...},\n",
    "    \"files\": [],\n",
    "    \"mentions\": [\n",
    "      {\n",
    "        \"@context\": {...},\n",
    "        \"@type\": \"Mention\",\n",
    "        \"id\": \"8547f62b-d310-4049-b355-aca6824a2de6\",\n",
    "        \"segment\": [\n",
    "          {\n",
    "            \"@context\": {...},\n",
    "            \"@type\": \"Index\",\n",
    "            \"container_id\": \"859064d1-5b01-4dbf-8f17-98d69d6e62db\",\n",
    "            \"start\": 0,\n",
    "            \"stop\": 4,\n",
    "            \"_py_type\": \"emissor.representation.container-Index\"\n",
    "          }\n",
    "        ],\n",
    "        \"annotations\": [\n",
    "          {\n",
    "            \"@context\": {...},\n",
    "            \"@type\": \"Annotation\",\n",
    "            \"type\": \"ConversationalAgent\",\n",
    "            \"value\": \"ACTION\",\n",
    "            \"source\": \"LEOLANI\",\n",
    "            \"timestamp\": 1730187274335,\n",
    "            \"_py_type\": \"emissor.representation.scenario-Annotation\"\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    ],\n",
    "    \"text\": \"Look\"\n",
    "  },\n",
    "```\n",
    "\n",
    "The meta data for the images are saved in ```image.json```, the image itself in which an object was detected is saved in a subdirectory \"image\", where each image file has the name of the object. Here is the representation fio an aimage signal in the ````image.json``` meta file:\n",
    "\n",
    "```\n",
    "  {\n",
    "    \"@context\": {...},\n",
    "    \"@type\": \"ImageSignal\",\n",
    "    \"id\": \"3eca43c8-a9a8-4743-a20d-85badb1f13ff\",\n",
    "    \"ruler\": {\n",
    "      \"@context\": {...},\n",
    "      \"@type\": \"MultiIndex\",\n",
    "      \"container_id\": \"3eca43c8-a9a8-4743-a20d-85badb1f13ff\",\n",
    "      \"bounds\": [\n",
    "        0,\n",
    "        0,\n",
    "        640,\n",
    "        480\n",
    "      ],\n",
    "      \"_py_type\": \"emissor.representation.container-MultiIndex\"\n",
    "    },\n",
    "    \"array\": \"\",\n",
    "    \"modality\": \"IMAGE\",\n",
    "    \"time\": {\n",
    "      \"@context\": {...},\n",
    "      \"@type\": \"TemporalRuler\",\n",
    "      \"container_id\": \"cc1d9b3d-dcb2-466e-9b56-aae0944453fe\",\n",
    "      \"start\": 1730191697555,\n",
    "      \"end\": 1730191697555\n",
    "    },\n",
    "    \"files\": [\n",
    "      \"./emissor/cc1d9b3d-dcb2-466e-9b56-aae0944453fe/image/Apple_f33eaaa0.jpg\"\n",
    "    ],\n",
    "    \"mentions\": [\n",
    "      {\n",
    "        \"@context\": {...},\n",
    "        \"@type\": \"Mention\",\n",
    "        \"id\": \"7607d8ef-c67f-4a66-a28b-84f2f2a14bb4\",\n",
    "        \"segment\": [\n",
    "          {\n",
    "            \"@context\": {...},\n",
    "            \"@type\": \"MultiIndex\",\n",
    "            \"container_id\": \"3eca43c8-a9a8-4743-a20d-85badb1f13ff\",\n",
    "            \"bounds\": [\n",
    "              0,\n",
    "              0,\n",
    "              -1,\n",
    "              0\n",
    "            ],\n",
    "            \"_py_type\": \"emissor.representation.container-MultiIndex\"\n",
    "          }\n",
    "        ],\n",
    "        \"annotations\": [\n",
    "          {\n",
    "            \"@context\": {...},\n",
    "            \"@type\": \"Annotation\",\n",
    "            \"type\": \"apple\",\n",
    "            \"value\": {\n",
    "              \"_py_type\": \"builtins-dict\"\n",
    "            },\n",
    "            \"source\": \"Ai2Thor\",\n",
    "            \"timestamp\": 1730191697,\n",
    "            \"_py_type\": \"emissor.representation.scenario-Annotation\"\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262980c6-1bfb-413a-9296-1c3b2dac0de5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
