{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c045b6f-2966-40da-bbf0-2cca52370a3b",
   "metadata": {},
   "source": [
    "## Llama chats with Blender"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4b8c17-89bd-41d3-82fb-5999f9c08d83",
   "metadata": {},
   "source": [
    "This notebook shows how Llama chats with Blender through an EMISSOR client. The EMISSOR layer will capture the interaction as a scenario for further analysis. \n",
    "\n",
    "https://github.com/ollama/ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35b76e5-6d8a-4091-b529-802049c5e39b",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa66457f-d098-46b4-aca3-876400dcc702",
   "metadata": {},
   "source": [
    "* pip install emissor\n",
    "* pip install cltl.combots\n",
    "* pip install ollama\n",
    "* pip install -U langchain-ollama\n",
    "* pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63864de-b1c2-4051-9a2f-23bc2dbb84ee",
   "metadata": {},
   "source": [
    "### Loading Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f54334a9-b0bc-440d-868d-924a772b9620",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "llama_model = \"llama3.2:1b\" ### 1B\n",
    "#llama_model = \"llama3.2\" ### 3B\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model = llama_model,\n",
    "    temperature = 0.8,\n",
    "    num_predict = 256,\n",
    "    # other params ...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e96a772-4705-4291-941e-909658eae39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct = { 'role': 'system', 'content': \"You are a docter and you will receive questions from patients. Be brief and no more than two sentences.\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef38888-5f85-4a9e-aa02-9e2e34886337",
   "metadata": {},
   "source": [
    "### Loading Blender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3c5930c-b29e-4987-bf24-bbf7fdec63de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlenderbotTokenizer, BlenderbotForConditionalGeneration\n",
    "mname = 'facebook/blenderbot-400M-distill'\n",
    "blender_model = BlenderbotForConditionalGeneration.from_pretrained(mname)\n",
    "blender_tokenizer = BlenderbotTokenizer.from_pretrained(mname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8109f584-6009-47f3-9333-19b09978f355",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size = 5\n",
    "def get_answer_from_blender(prompt:str, history_list:[]):\n",
    "    answer = \"\"\n",
    "    sentences = []\n",
    "    history = \"\"\n",
    "    for i, his in enumerate(history):\n",
    "        if i==context_size:\n",
    "            break\n",
    "        history += his['content'] +\". \"\n",
    "    input_prompt = history+prompt\n",
    "    bot_input_ids = blender_tokenizer(input_prompt, return_tensors='pt')\n",
    "    chat_history_ids = blender_model.generate(**bot_input_ids)\n",
    "    utteranceList = blender_tokenizer.batch_decode(chat_history_ids)\n",
    "    answer = utteranceList[0].strip('</s>')\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7387c124-7772-4511-9afb-cb83449ca47a",
   "metadata": {},
   "source": [
    "### Creating an EMISSOR client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89a29313-9cd6-4321-9f0d-fd2746ae2ac6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from leolani_client import LeolaniChatClient\n",
    "emissor_path = \"./emissor\"\n",
    "HUMAN=\"BlenderBot\"\n",
    "AGENT=\"Llama\"\n",
    "leolaniClient = LeolaniChatClient(emissor_path=emissor_path, agent=AGENT, human=HUMAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f437945-4b5c-418a-b9a1-55b7435d5da6",
   "metadata": {},
   "source": [
    "### Interaction loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa6d0c3-a5fa-4a5a-b2f9-093ee36aba51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are a docter and you will receive questions from patients. Be brief and no more than two sentences.'}]\n",
      "Llama: <|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "I'm ready to answer your questions as a doctor. What's your first question?\n",
      "\n",
      "\tBlenderBot:  My first question would be whether or not I should go to the dentist.\n",
      "Llama: It's great that you're thinking about taking care of your oral health. Generally, regular dental check-ups can help catch any potential problems early on and prevent more serious issues from developing, so it might be a good idea to schedule an appointment with your dentist as soon as possible to get your teeth checked out.\n",
      "\n",
      "\tBlenderBot:  That's a great idea, thank you! I'll make sure to do that asap.\n",
      "Llama: It's always better to prioritize your oral health, and regular check-ups can provide valuable peace of mind and help identify any potential problems early on. By scheduling an appointment with your dentist, you'll be taking the first step towards maintaining healthy teeth and a beautiful smile for years to come.\n",
      "\n",
      "\tBlenderBot:  Thank you for the advice. I will make sure to keep that in mind next time I need a check-up.\n",
      "Llama: It's always a good idea to prioritize your oral health and schedule regular dental check-ups to catch any potential problems early on.\n",
      "\n",
      "Just one more quick question - would you like me to provide you with some general tips or recommendations for maintaining healthy teeth and gums, such as brushing and flossing routines?\n",
      "\n",
      "\tBlenderBot:  Yes, that would be great! I would love to know more about preventing oral diseases such as tooth decay and cavities.\n"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "history.append(instruct)\n",
    "print(history)\n",
    "### First prompt\n",
    "response = llm.invoke(history)\n",
    "utterance = response.content\n",
    "print(AGENT + \": \" + utterance)\n",
    "leolaniClient._add_utterance(AGENT, utterance) \n",
    "prompt = { 'role': 'system', 'content': utterance}\n",
    "history.append(prompt)\n",
    "\n",
    "utterance = get_answer_from_blender(utterance, history)\n",
    "print('\\n\\t'+HUMAN + \": \" + utterance)\n",
    "prompt = { 'role': 'user', 'content': utterance}\n",
    "history.append(prompt)\n",
    "leolaniClient._add_utterance(AGENT, prompt)\n",
    "\n",
    "max_count = 5\n",
    "counter = 0\n",
    "\n",
    "while counter < max_count:\n",
    "    counter +=1\n",
    "    # Create the response from the system and store this as a new signal\n",
    "    response = llm.invoke(history)\n",
    "    utterance = response.content\n",
    "    print(AGENT + \": \" + utterance)\n",
    "    leolaniClient._add_utterance(AGENT, utterance) \n",
    "    prompt = { 'role': 'system', 'content': utterance}\n",
    "    history.append(prompt)\n",
    "\n",
    "    utterance = get_answer_from_blender(utterance, history)\n",
    "    print('\\n\\t'+HUMAN + \": \" + utterance)\n",
    "    prompt = { 'role': 'user', 'content': utterance}\n",
    "    history.append(prompt)\n",
    "    leolaniClient._add_utterance(AGENT, prompt)\n",
    "\n",
    "##### After completion, we save the scenario in the defined emissor folder.\n",
    "leolaniClient._save_scenario() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1136a7a-2284-46ff-80b3-00247195e022",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
